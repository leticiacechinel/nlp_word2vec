{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5f270cbf-9b6e-4203-bd25-809ea64ab7f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/leticia/Documents/estudos\n"
     ]
    }
   ],
   "source": [
    "! pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2de0e7e4-a3e8-480b-90aa-c96bcaac5c74",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim.models\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e626be80-7c16-453c-a640-963d51464c77",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = corpus = [\n",
    "    [\"o\", \"gato\", \"está\", \"no\", \"telhado\"],\n",
    "    [\"o\", \"cachorro\", \"correu\", \"pela\", \"rua\"],\n",
    "    [\"a\", \"chuva\", \"molhou\", \"os\", \"carros\"]\n",
    "]\n",
    "model = gensim.models.Word2Vec(sentences=sentences, min_count=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c7a34117-0722-4696-939e-8eef0f187d15",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('chuva', 0.2528940737247467),\n",
       " ('carros', 0.13725271821022034),\n",
       " ('a', 0.04411465302109718),\n",
       " ('rua', 0.012811630964279175),\n",
       " ('molhou', 0.00659846980124712),\n",
       " ('telhado', -0.0011978286784142256),\n",
       " ('pela', -0.02546103298664093),\n",
       " ('o', -0.041253410279750824),\n",
       " ('está', -0.0763891264796257),\n",
       " ('cachorro', -0.10619831830263138)]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv.most_similar(\"gato\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e026276e-0ece-4f67-8fd2-a8d9cca7376d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "36bfc833-50b5-4ab4-8fe7-5a983fe076e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reviews.csv  Reviews.csv.zip  TCC_Leticia_C.pdf  word2vec.ipynb\n"
     ]
    }
   ],
   "source": [
    "! ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f1e44b05-d7ab-4911-a326-ab009b53165b",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"Reviews.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "369b77ec-4c2b-4273-8635-49faaee29c47",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>ProductId</th>\n",
       "      <th>UserId</th>\n",
       "      <th>ProfileName</th>\n",
       "      <th>HelpfulnessNumerator</th>\n",
       "      <th>HelpfulnessDenominator</th>\n",
       "      <th>Score</th>\n",
       "      <th>Time</th>\n",
       "      <th>Summary</th>\n",
       "      <th>Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>B001E4KFG0</td>\n",
       "      <td>A3SGXH7AUHU8GW</td>\n",
       "      <td>delmartian</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>1303862400</td>\n",
       "      <td>Good Quality Dog Food</td>\n",
       "      <td>I have bought several of the Vitality canned d...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>B00813GRG4</td>\n",
       "      <td>A1D87F6ZCVE5NK</td>\n",
       "      <td>dll pa</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1346976000</td>\n",
       "      <td>Not as Advertised</td>\n",
       "      <td>Product arrived labeled as Jumbo Salted Peanut...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>B000LQOCH0</td>\n",
       "      <td>ABXLMWJIXXAIN</td>\n",
       "      <td>Natalia Corres \"Natalia Corres\"</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>1219017600</td>\n",
       "      <td>\"Delight\" says it all</td>\n",
       "      <td>This is a confection that has been around a fe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>B000UA0QIQ</td>\n",
       "      <td>A395BORC6FGVXV</td>\n",
       "      <td>Karl</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>1307923200</td>\n",
       "      <td>Cough Medicine</td>\n",
       "      <td>If you are looking for the secret ingredient i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>B006K2ZZ7K</td>\n",
       "      <td>A1UQRSCLF8GW1T</td>\n",
       "      <td>Michael D. Bigham \"M. Wassir\"</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>1350777600</td>\n",
       "      <td>Great taffy</td>\n",
       "      <td>Great taffy at a great price.  There was a wid...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Id   ProductId          UserId                      ProfileName  \\\n",
       "0   1  B001E4KFG0  A3SGXH7AUHU8GW                       delmartian   \n",
       "1   2  B00813GRG4  A1D87F6ZCVE5NK                           dll pa   \n",
       "2   3  B000LQOCH0   ABXLMWJIXXAIN  Natalia Corres \"Natalia Corres\"   \n",
       "3   4  B000UA0QIQ  A395BORC6FGVXV                             Karl   \n",
       "4   5  B006K2ZZ7K  A1UQRSCLF8GW1T    Michael D. Bigham \"M. Wassir\"   \n",
       "\n",
       "   HelpfulnessNumerator  HelpfulnessDenominator  Score        Time  \\\n",
       "0                     1                       1      5  1303862400   \n",
       "1                     0                       0      1  1346976000   \n",
       "2                     1                       1      4  1219017600   \n",
       "3                     3                       3      2  1307923200   \n",
       "4                     0                       0      5  1350777600   \n",
       "\n",
       "                 Summary                                               Text  \n",
       "0  Good Quality Dog Food  I have bought several of the Vitality canned d...  \n",
       "1      Not as Advertised  Product arrived labeled as Jumbo Salted Peanut...  \n",
       "2  \"Delight\" says it all  This is a confection that has been around a fe...  \n",
       "3         Cough Medicine  If you are looking for the secret ingredient i...  \n",
       "4            Great taffy  Great taffy at a great price.  There was a wid...  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b24a2457-22cc-4f90-9b4f-2c15245ad082",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "568454"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f345b4e-2bbd-4854-93f0-5d780f4990ef",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a87f094d-2b76-4845-aa16-9264977349f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = []\n",
    "\n",
    "for texto in data.Text:\n",
    "    sentences.append(texto.split(\" \"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "46cfc0d8-2a9a-4d8e-9c85-a56cd4525b34",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Carregar o CSV\n",
    "df = pd.read_csv(\"Reviews.csv\")\n",
    "\n",
    "# Selecionar a coluna com as reviews\n",
    "reviews = df[\"Text\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a53ec733-7a8d-492b-970b-08840347342a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting nltk\n",
      "  Downloading nltk-3.9.1-py3-none-any.whl.metadata (2.9 kB)\n",
      "Collecting click (from nltk)\n",
      "  Using cached click-8.1.7-py3-none-any.whl.metadata (3.0 kB)\n",
      "Collecting joblib (from nltk)\n",
      "  Using cached joblib-1.4.2-py3-none-any.whl.metadata (5.4 kB)\n",
      "Collecting regex>=2021.8.3 (from nltk)\n",
      "  Downloading regex-2024.11.6-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (40 kB)\n",
      "Collecting tqdm (from nltk)\n",
      "  Downloading tqdm-4.67.1-py3-none-any.whl.metadata (57 kB)\n",
      "Downloading nltk-3.9.1-py3-none-any.whl (1.5 MB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.5/1.5 MB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading regex-2024.11.6-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (781 kB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m781.7/781.7 kB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hUsing cached click-8.1.7-py3-none-any.whl (97 kB)\n",
      "Using cached joblib-1.4.2-py3-none-any.whl (301 kB)\n",
      "Downloading tqdm-4.67.1-py3-none-any.whl (78 kB)\n",
      "Installing collected packages: tqdm, regex, joblib, click, nltk\n",
      "Successfully installed click-8.1.7 joblib-1.4.2 nltk-3.9.1 regex-2024.11.6 tqdm-4.67.1\n"
     ]
    }
   ],
   "source": [
    "! pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "6c13e2f2-c752-4936-9ed6-7bacf0e67c01",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/leticia/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/leticia/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# Baixar recursos necessários do NLTK\n",
    "nltk.download(\"punkt\")\n",
    "nltk.download(\"stopwords\")\n",
    "\n",
    "stop_words = set(stopwords.words(\"english\"))\n",
    "\n",
    "def preprocess_text(text):\n",
    "    # Converter para minúsculas\n",
    "    text = text.lower()\n",
    "    # Remover pontuações e números\n",
    "    text = re.sub(r\"[^\\w\\s]\", \"\", text)\n",
    "    text = re.sub(r\"\\d+\", \"\", text)\n",
    "    # Tokenizar em palavras\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "    # Remover stopwords\n",
    "    tokens = [word for word in tokens if word not in stop_words]\n",
    "    return tokens\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d2baf166-dd05-4c8d-8a93-87889641414f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     /home/leticia/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    [bought, several, vitality, canned, dog, food,...\n",
      "1    [product, arrived, labeled, jumbo, salted, pea...\n",
      "2    [confection, around, centuries, light, pillowy...\n",
      "3    [looking, secret, ingredient, robitussin, beli...\n",
      "4    [great, taffy, great, price, wide, assortment,...\n",
      "Name: Text, dtype: object\n"
     ]
    }
   ],
   "source": [
    "# Aplicar o pré-processamento em cada review\n",
    "nltk.download('punkt_tab')\n",
    "\n",
    "processed_reviews = reviews.apply(preprocess_text)\n",
    "\n",
    "# Exemplo de saída\n",
    "print(processed_reviews.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "2d4104bc-98d3-476d-9fea-c3addd9efafa",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = processed_reviews.tolist()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "8ee48542-a4b7-4538-8641-181c784c5743",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = gensim.models.Word2Vec(sentences=corpus, min_count=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "fa640e26-f67f-45a4-9bfc-62ff7d4318f0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('foodbr', 0.8165464401245117),\n",
       " ('foods', 0.7421149611473083),\n",
       " ('kibble', 0.688446581363678),\n",
       " ('iams', 0.6229534149169922),\n",
       " ('beneful', 0.6212493777275085),\n",
       " ('kibbles', 0.6206709742546082),\n",
       " ('wellness', 0.6063350439071655),\n",
       " ('weruva', 0.5924379229545593),\n",
       " ('sojos', 0.5838373899459839),\n",
       " ('organix', 0.5712035298347473)]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv.most_similar(\"food\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "e5667ea5-7625-4729-940b-fcc8f4cfe1b8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('great', 0.8224396705627441),\n",
       " ('decent', 0.79594886302948),\n",
       " ('goodbr', 0.7004961967468262),\n",
       " ('bad', 0.6776430606842041),\n",
       " ('awesome', 0.6776024699211121),\n",
       " ('excellent', 0.6661296486854553),\n",
       " ('nice', 0.6543703079223633),\n",
       " ('fantastic', 0.6515397429466248),\n",
       " ('alright', 0.616104245185852),\n",
       " ('terrific', 0.5993726253509521)]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv.most_similar(\"good\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3c9b7a8-4d9f-4b88-8eb1-06e49cd31550",
   "metadata": {},
   "source": [
    "### o word2vec parece estar funcionando de acordo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ed57995-79c0-4093-861f-5ca1adf40439",
   "metadata": {},
   "source": [
    "para formatar o texto de maneira que seja aceito pela lstm precisamos transformar cada palavra em vetor, criar uma sequencia de um tamanho arbitrário onde X é uma sequencia de palavras e y é a palavra seguinte "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "cc852725-f67c-42ee-94e4-d067abfbfdc6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python 3.10.0\n"
     ]
    }
   ],
   "source": [
    "! python --version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "9f0b5676-f7a7-4443-924d-d596c8b2b74c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU disponível: [PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "print(\"GPU disponível:\", tf.config.list_physical_devices('GPU'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "b1a55629-ea34-471a-a327-e7a76a10aa49",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "import numpy as np\n",
    "\n",
    "# Definir o tamanho da janela de sequência\n",
    "SEQUENCE_LENGTH = 10\n",
    "\n",
    "# Mapear palavras para índices e vice-versa\n",
    "word_vectors = model.wv  # Modelo Word2Vec treinado\n",
    "word_index = {word: i for i, word in enumerate(word_vectors.index_to_key)}\n",
    "index_word = {i: word for word, i in word_index.items()}\n",
    "\n",
    "# Gerar sequências de palavras e próximo token\n",
    "sequences = []\n",
    "next_words = []\n",
    "\n",
    "for review in corpus:\n",
    "    for i in range(SEQUENCE_LENGTH, len(review)):\n",
    "        # Cria a sequência de entrada e a palavra alvo\n",
    "        seq = review[i-SEQUENCE_LENGTH:i]\n",
    "        target = review[i]\n",
    "\n",
    "        # Ignorar sequências com palavras fora do vocabulário\n",
    "        if all(word in word_index for word in seq) and target in word_index:\n",
    "            sequences.append([word_index[word] for word in seq])\n",
    "            next_words.append(word_index[target])\n",
    "\n",
    "# Converter para arrays NumPy\n",
    "X = np.array(sequences)\n",
    "y = np.array(next_words)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "9460a86a-497a-4ddb-8785-4d1bddd91258",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "17527399"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "fb795fe5-b768-4aa1-b19a-aa333bba82e5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_1\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential_1\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ embedding_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)         │ ?                      │    <span style=\"color: #00af00; text-decoration-color: #00af00\">22,434,300</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ lstm_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)                   │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ embedding_1 (\u001b[38;5;33mEmbedding\u001b[0m)         │ ?                      │    \u001b[38;5;34m22,434,300\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ lstm_1 (\u001b[38;5;33mLSTM\u001b[0m)                   │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_1 (\u001b[38;5;33mDense\u001b[0m)                 │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">22,434,300</span> (85.58 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m22,434,300\u001b[0m (85.58 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">22,434,300</span> (85.58 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m22,434,300\u001b[0m (85.58 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense\n",
    "\n",
    "# Parâmetros do modelo\n",
    "EMBEDDING_DIM = word_vectors.vector_size\n",
    "VOCAB_SIZE = len(word_vectors.index_to_key)\n",
    "\n",
    "# Construção do modelo\n",
    "model_lstm = Sequential([\n",
    "    Embedding(input_dim=VOCAB_SIZE, output_dim=EMBEDDING_DIM, weights=[word_vectors.vectors], input_length=SEQUENCE_LENGTH, trainable=False),\n",
    "    LSTM(128, return_sequences=False),\n",
    "    Dense(VOCAB_SIZE, activation=\"softmax\")\n",
    "])\n",
    "\n",
    "model_lstm.compile(optimizer=\"adam\", loss=\"sparse_categorical_crossentropy\", metrics=[\"accuracy\"])\n",
    "model_lstm.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "691d0cd3-103f-40e5-88e0-8d86fd9763e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001b[1m219093/219093\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9349s\u001b[0m 43ms/step - accuracy: 0.0852 - loss: 7.0040 - val_accuracy: 0.0965 - val_loss: 6.8243\n",
      "Epoch 2/10\n",
      "\u001b[1m219093/219093\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9368s\u001b[0m 43ms/step - accuracy: 0.1018 - loss: 6.7146 - val_accuracy: 0.0976 - val_loss: 6.7912\n",
      "Epoch 3/10\n",
      "\u001b[1m219093/219093\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9293s\u001b[0m 42ms/step - accuracy: 0.1030 - loss: 6.6728 - val_accuracy: 0.0974 - val_loss: 6.7824\n",
      "Epoch 4/10\n",
      "\u001b[1m219093/219093\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9206s\u001b[0m 42ms/step - accuracy: 0.1023 - loss: 6.6563 - val_accuracy: 0.0958 - val_loss: 6.8050\n",
      "Epoch 5/10\n",
      "\u001b[1m219093/219093\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8879s\u001b[0m 41ms/step - accuracy: 0.1020 - loss: 6.6573 - val_accuracy: 0.0953 - val_loss: 6.8198\n",
      "Epoch 6/10\n",
      "\u001b[1m219093/219093\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8851s\u001b[0m 40ms/step - accuracy: 0.1012 - loss: 6.6472 - val_accuracy: 0.0942 - val_loss: 6.8510\n",
      "Epoch 7/10\n",
      "\u001b[1m219093/219093\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8794s\u001b[0m 40ms/step - accuracy: 0.0988 - loss: 6.6583 - val_accuracy: 0.0932 - val_loss: 6.8752\n",
      "Epoch 8/10\n",
      "\u001b[1m219093/219093\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8723s\u001b[0m 40ms/step - accuracy: 0.0971 - loss: 6.6740 - val_accuracy: 0.0911 - val_loss: 6.9052\n",
      "Epoch 9/10\n",
      "\u001b[1m219093/219093\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8774s\u001b[0m 40ms/step - accuracy: 0.0962 - loss: 6.6764 - val_accuracy: 0.0914 - val_loss: 6.8972\n",
      "Epoch 10/10\n",
      "\u001b[1m219093/219093\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8770s\u001b[0m 40ms/step - accuracy: 0.0958 - loss: 6.6605 - val_accuracy: 0.0921 - val_loss: 6.8909\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x70932eb78850>"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_lstm.fit(X, y, epochs=10, batch_size=64, validation_split=0.2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "9ed17109-6276-4cbb-9cd9-078ec2edbf47",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Próxima palavra: treatim\n"
     ]
    }
   ],
   "source": [
    "def predict_next_word(model, sequence, word_index, index_word):\n",
    "    # Converter sequência de palavras para índices\n",
    "    seq_indices = [word_index[word] for word in sequence if word in word_index]\n",
    "    if len(seq_indices) < SEQUENCE_LENGTH:\n",
    "        seq_indices = [0] * (SEQUENCE_LENGTH - len(seq_indices)) + seq_indices\n",
    "    seq_indices = np.array(seq_indices).reshape(1, -1)\n",
    "\n",
    "    # Fazer a previsão\n",
    "    prediction = model.predict(seq_indices, verbose=0)\n",
    "    predicted_index = np.argmax(prediction)\n",
    "    return index_word[predicted_index]\n",
    "\n",
    "# Exemplo de uso\n",
    "sequence = [\"i\", \"love\", \"this\", \"spicy\", \"food\"]  # Últimos 5 tokens de exemplo\n",
    "next_word = predict_next_word(model_lstm, sequence, word_index, index_word)\n",
    "print(f\"Próxima palavra: {next_word}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "36f3b96b-353f-43ac-9a3e-267dc08a5dd2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Próxima palavra: well\n"
     ]
    }
   ],
   "source": [
    "def predict_next_word(model, sequence, word_index, index_word):\n",
    "    # Converter sequência de palavras para índices\n",
    "    seq_indices = [word_index[word] for word in sequence if word in word_index]\n",
    "    if len(seq_indices) < SEQUENCE_LENGTH:\n",
    "        seq_indices = [0] * (SEQUENCE_LENGTH - len(seq_indices)) + seq_indices\n",
    "    seq_indices = np.array(seq_indices).reshape(1, -1)\n",
    "\n",
    "    # Fazer a previsão\n",
    "    prediction = model.predict(seq_indices, verbose=0)\n",
    "    predicted_index = np.argmax(prediction)\n",
    "    return index_word[predicted_index]\n",
    "\n",
    "# Exemplo de uso\n",
    "sequence = [\"this\", \"model\", \"doesnt\", \"work\", \"pretty\"]  # Últimos 5 tokens de exemplo\n",
    "next_word = predict_next_word(model_lstm, sequence, word_index, index_word)\n",
    "print(f\"Próxima palavra: {next_word}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fa7848d-ee27-4209-a7cf-abc677d5c7f8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
